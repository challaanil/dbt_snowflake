import boto3
import json
import smbclient
import datetime
from zoneinfo import ZoneInfo
import socket
import os
import logging
import time

# Setting logger
msg_format = '%(asctime)s %(levelname)s %(name)s: %(message)s'
datetime_format = '%Y-%m-%d %H:%M:%S'
logging.basicConfig(format=msg_format, datefmt=datetime_format)
logger = logging.getLogger()
logger.setLevel(logging.INFO)

job_name = "lambda_job"

def get_credentials(secret_name):
    secretsmanager_client = boto3.client('secretsmanager', region_name='us-west-2')
    try:
        logger.info(f"Fetching secret for: {secret_name}")
        get_secret_value_response = secretsmanager_client.get_secret_value(SecretId=secret_name)
        
        if 'SecretString' in get_secret_value_response:
            secret = get_secret_value_response['SecretString']
            secret_dict = json.loads(secret)
            logger.info("Successfully fetched secret as string")
            return secret_dict
        else:
            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])
            secret_dict = json.loads(decoded_binary_secret)
            logger.info("Successfully fetched secret as binary.")
            return secret_dict
    except Exception as e:
        logger.error(f"Error fetching secret: {e}", exc_info=True)
        return None

def configure_smb(smb_username, smb_password):
    try:
        if smb_username and smb_password:
            smbclient.ClientConfig(username=smb_username, password=smb_password)
            logger.info(f"SMB client configured successfully with username: {smb_username}")
        else:
            logger.error("SMB credentials are missing. Cannot configure SMB client.")
    except Exception as e:
        logger.error(f"Failed to configure SMB client: {e}", exc_info=True)

def is_port_open(host, port):
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(60)
    try:
        logger.info(f"Checking if port {port} on host {host} is open.")
        result = sock.connect_ex((host, port))
        if result == 0:
            logger.info(f"Port {port} on host {host} is open")
            return True
        else:
            logger.warning(f"Port {port} on host {host} is closed")
            return False
    except socket.error as e:
        logger.error(f"Socket error while checking port {port} on host {host}: {e}", exc_info=True)
        return False
    finally:
        sock.close()
        logger.info("Socket connection closed")

def create_smb_folder(smb_fileshare_path):
    try:
        if not smbclient.path.exists(smb_fileshare_path):
            smbclient.mkdir(smb_fileshare_path)
            logger.info(f"Created SMB folder: {smb_fileshare_path}")
        else:
            logger.info(f"SMB folder already exists: {smb_fileshare_path}")
    except Exception as e:
        logger.error(f"Failed to create SMB folder: {smb_fileshare_path}. Error: {e}", exc_info=True)

def file_with_timestamp(use_timestamp, files):
    try:
        # Check if the file path is valid and split the file name and extension
        file_parts = files.split('/')
        file_name = file_parts[-1]
        
        if use_timestamp:
            cst = ZoneInfo("America/Chicago")
            timestamp = datetime.datetime.now(cst).strftime("%Y%m%d%H%M%S")
            if '.' in file_name:
                name, ext = file_name.rsplit('.', 1)
                filename = f"{name}_{timestamp}.{ext}"
            else:
                filename = f"{file_name}_{timestamp}"
            logger.info(f"Generated file name with timestamp: {filename}")
        else:
            filename = file_name
            logger.info(f"Generated file name without timestamp: {filename}")
        
        return filename
    except Exception as e:
        logger.error(f"Failed to generate file name. Error: {e}", exc_info=True)
        return None

def log_transfer_duration(start_time, end_time):
    try:
        # Calculate the duration
        duration = end_time - start_time
        total_seconds = int(duration.total_seconds())
        hours, remainder = divmod(total_seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        load_duration = f"{hours}:{minutes:02}:{seconds:02}"  # Ensure minutes and seconds are two digits
        logger.info(f"Total duration for file transfer: {load_duration}")
        return load_duration
    except Exception as e:
        logger.error(f"Failed to log transfer duration. Error: {e}", exc_info=True)
        return None

def generate_audit_details(environment, source_system, batch_id_generation, smb_host, smb_port, bucket_name, s3_path, smb_fileshare_path, filename_with_timestamp, archival, archival_path, start_time, end_time, load_time_in_minutes, action, file_size, last_modified, error_message=None):
    audit_details = {
        "job_name": job_name,
        "environment": environment,
        "source_system": source_system,
        "batch_id_generation": batch_id_generation,
        "smb_host": smb_host,
        "smb_port": smb_port,
        "bucket_name": bucket_name,
        "s3_path": s3_path,
        "smb_fileshare_path": smb_fileshare_path,
        "filename_with_timestamp": filename_with_timestamp,
        "archival": archival,
        "archival_path": archival_path,
        "last_run_date": start_time.strftime("%Y-%m-%d %H:%M:%S"),
        "load_finish_date": end_time.strftime("%Y-%m-%d %H:%M:%S"),
        "load_time_in_minutes": load_time_in_minutes,
        "status": action,
        "file_size": file_size,
        "last_modified": last_modified.strftime("%Y-%m-%d %H:%M:%S") if last_modified else None,
        "archived": "archived" if archival else "not archived",
        "source_data_deleted": "deleted" if action == "deleted" else "not deleted",
        "error_message": error_message
    }
    return audit_details


def transfer_file_to_smb(bucket_name, s3_key, smb_fileshare_path, filename_with_timestamp, chunk_size_mb, smb_host, smb_port, archival, archival_path, source_system, batch_id_generation, environment):
    cst = ZoneInfo("America/Chicago")
    start_time = datetime.datetime.now(cst)
    logger.info(f"File Transfer started for {s3_key}.")
    transferred_files = []
    transfer_file_to_smb_s3_client = boto3.client('s3')
    try:
        s3_response = transfer_file_to_smb_s3_client.get_object(Bucket=bucket_name, Key=s3_key)
        file_stream = s3_response['Body']
        file_size = s3_response['ContentLength']
        last_modified = s3_response['LastModified']
        max_size_mb = 50000 * 1024 * 1024
        if file_size == 0 or file_size > max_size_mb:
            msg = f"Skipping the file process where file size is: {file_size} bytes,where not matching with file validation {max_size_mb} ."
            logger.warning(msg)
            end_time = datetime.datetime.now(cst)
            load_time_in_minutes = log_transfer_duration(start_time, end_time)
            audit_details = generate_audit_details(environment, source_system, batch_id_generation, smb_host, smb_port, bucket_name, s3_key, smb_fileshare_path, filename_with_timestamp, archival, archival_path, start_time, end_time, load_time_in_minutes, "failed", file_size, last_modified, msg)
            return transferred_files, audit_details, msg
        
        filename = file_with_timestamp(filename_with_timestamp, s3_key)
        smb_file_path = f"{smb_fileshare_path}\\{filename}"
        logger.info(f"SMB file path: {smb_file_path}")
        chunk_size = chunk_size_mb * 1024 * 1024
        
        with smbclient.open_file(smb_file_path, mode='w') as smb_file:
            logger.info(f"Transferring {s3_key} to {smb_file_path} in chunks of {chunk_size_mb} MB.")
            while True:
                try:
                    chunk = file_stream.read(chunk_size).decode('utf-8')
                    if not chunk:
                        break
                    smb_file.write(chunk)
                    logger.debug(f"Writing chunk of size: {len(chunk)} bytes.")
                except Exception as e:
                    logger.error(f"Error while writing chunk for {s3_key}: {e}", exc_info=True)
                    end_time = datetime.datetime.now(cst)
                    load_time_in_minutes = log_transfer_duration(start_time, end_time)
                    audit_details = generate_audit_details(environment, source_system, batch_id_generation, smb_host, smb_port, bucket_name, s3_key, smb_fileshare_path, filename_with_timestamp, archival, archival_path, start_time, end_time, load_time_in_minutes, "failed", file_size, last_modified, str(e))
                    return transferred_files, audit_details, str(e)
        
        transferred_files.append(s3_key)
        end_time = datetime.datetime.now(cst)
        logger.info(f"File transferred for {s3_key}.")
        load_time_in_minutes = log_transfer_duration(start_time, end_time)
        audit_details = generate_audit_details(environment, source_system, batch_id_generation, smb_host, smb_port, bucket_name, s3_key, smb_file_path, filename_with_timestamp, archival, archival_path, start_time, end_time, load_time_in_minutes, "success", file_size, last_modified)
        return transferred_files, audit_details, None
    except Exception as e:
        logger.error(f"Error in transferring file {s3_key}: {e}", exc_info=True)
        end_time = datetime.datetime.now(cst)
        load_time_in_minutes = log_transfer_duration(start_time, end_time)
        audit_details = generate_audit_details(environment, source_system, batch_id_generation, smb_host, smb_port, bucket_name, s3_key, smb_fileshare_path, filename_with_timestamp, archival, archival_path, start_time, end_time, load_time_in_minutes, "failed", 0, None, str(e))
        return transferred_files, audit_details, str(e)


def transfer_folder_to_smb(bucket_name, s3_folder, smb_fileshare_path, filename_with_timestamp, chunk_size_mb, smb_host, smb_port, archival, archival_path, source_system, batch_id_generation, environment):
    transfer_folder_to_smb_s3_client = boto3.client('s3')
    transferred_files = []
    audit_details_list = []
    failed_files = []
    try:
        logger.info(f"Starting transfer of s3 folder '{s3_folder}' to SMB path '{smb_fileshare_path}' .")
        paginator = transfer_folder_to_smb_s3_client.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=bucket_name, Prefix=s3_folder):
            for obj in page.get('Contents', []):
                s3_key = obj['Key']
                if s3_key.endswith('/'):
                    logger.info(f"Skipping folder: {s3_key}")
                    continue
                logger.info(f"Transferring file: {s3_key}")
                files, audit_details, error = transfer_file_to_smb(bucket_name, s3_key, smb_fileshare_path, filename_with_timestamp, chunk_size_mb, smb_host, smb_port, archival, archival_path, source_system, batch_id_generation, environment)
                transferred_files.extend(files)
                if audit_details:
                    audit_details_list.append(audit_details)
                if error:
                    failed_files.append((s3_key, error))
        logger.info(f"Successfully transferred all files from S3 folder '{s3_folder}' to SMB path '{smb_fileshare_path}' .")
    except Exception as e:
        logger.error(f"An error occurred during folder transfer from S3 folder '{s3_folder}' to SMB: {e}", exc_info=True)
        failed_files.append((s3_folder, str(e)))
    return transferred_files, audit_details_list, failed_files

def transfer_s3_to_smb(bucket_name, s3_path, smb_fileshare_path, filename_with_timestamp, chunk_size_mb, smb_host, smb_port, archival, archival_path, source_system, batch_id_generation, environment):
    transferred_files = []
    audit_details_list = []
    failed_files = []
    
    try:
        if s3_path.endswith("/"):
            logger.info(f"Handling folder transfer for S3 path: {s3_path}")
            files, audits, failures = transfer_folder_to_smb(
                bucket_name, s3_path, smb_fileshare_path, filename_with_timestamp, chunk_size_mb, smb_host, smb_port, 
                archival, archival_path, source_system, batch_id_generation, environment
            )
        else:
            logger.info(f"Handling file transfer for S3 key: {s3_path}")
            files, audit_details, error = transfer_file_to_smb(
                bucket_name, s3_path, smb_fileshare_path, filename_with_timestamp, chunk_size_mb, smb_host, smb_port, 
                archival, archival_path, source_system, batch_id_generation, environment
            )
            audits = [audit_details] if audit_details else []
            failures = [(s3_path, error)] if error else []
        transferred_files.extend(files)
        audit_details_list.extend(audits)
        failed_files.extend(failures)
    except Exception as e:
        logger.error(f"An error occurred during S3 to SMB transfer for path '{s3_path}': {e}", exc_info=True)
    return transferred_files, audit_details_list, failed_files

def delete_old_archives(use_retention, retention_period, bucket_name, archival_path, all_audit_details, source_system, batch_id_generation, environment):
    logger.info(f"Deleting archiving process enabled.\nBucket: {bucket_name}\nFolder: {archival_path}")
    if use_retention:
        cst = ZoneInfo("America/Chicago")
        delete_old_archives_s3_client = boto3.client('s3')
        paginator = delete_old_archives_s3_client.get_paginator('list_objects_v2')
        current_date = datetime.datetime.now(cst)
        logger.info(f"Deleting archiving process current date: {current_date}")
        try:
            for page in paginator.paginate(Bucket=bucket_name, Prefix=archival_path, Delimiter='/'):
                if 'CommonPrefixes' in page:
                    for folder in page['CommonPrefixes']:
                        folder_key = folder['Prefix']
                        logger.info(f"Checking folder: {folder_key}")
                        if '/' in folder_key:
                            folder_timestamp = folder_key.strip("/").split("/")[-1]
                            logger.info(f"Folder timestamp: {folder_timestamp}")
                            try:
                                folder_date = datetime.datetime.strptime(folder_timestamp, "%Y%m%d%H%M%S")
                                logger.info(f"Folder date: {folder_date}")
                                folder_age_days = (current_date - folder_date).days
                                logger.info(f"Folder age in days: {folder_age_days}")
                                if folder_age_days >= retention_period:
                                    logger.info(f"Folder {folder_key} is {folder_age_days} days old. Older than retention_period {retention_period} days.")
                                    delete_old_archives_s3_client.delete_object(Bucket=bucket_name, Key=folder_key)
                                    logger.info(f"Deleted folder: {folder_key}")
                                    end_time = datetime.datetime.now(cst)
                                    load_time_in_minutes = log_transfer_duration(current_date, end_time)
                                    delete_audit_details = generate_audit_details(environment, source_system, batch_id_generation, "", "", bucket_name, folder_key, "", "", "", current_date, end_time, load_time_in_minutes, "Archival folder deleted", 0, None)
                                    logger.info(f"Deleted files details: {delete_audit_details}")
                                    all_audit_details.append(delete_audit_details)
                            except ValueError as e:
                                logger.warning(f"Invalid date format in folder name: {folder_key}. Error: {e}")
                        else:
                            logger.warning(f"Folder name does not contain valid timestamp: {folder_key}")
        except Exception as e:
            logger.error(f"Error while processing folders: {e}")
    else:
        logger.info(f"Deleting archiving folder is disabled. Skipping deletion...")

def archive_files(use_archive, bucket_name, s3_key, archive_path, all_audit_details, source_system, batch_id_generation, environment):
    try:
        cst = ZoneInfo("America/Chicago")
        start_time = datetime.datetime.now(cst)
        logger.info(f"Archive batch generation: {batch_id_generation}")
        if use_archive:
            logger.info(f"Archiving enabled. Processing file: {s3_key}")
            archive_files_s3_client = boto3.client('s3')
            if s3_key.endswith("/"):
                parent_folder = s3_key.split("/")[-2]
            else:
                parent_folder = s3_key.split("/")[-2]
            archive_folder = os.path.join(os.path.join(archive_path, batch_id_generation), parent_folder)
            logger.info(f"Archive folder: {archive_folder}")
            archive_file_key = os.path.join(archive_folder, os.path.basename(s3_key))
            archive_files_s3_client.copy_object(
                Bucket=bucket_name,
                CopySource={'Bucket': bucket_name, 'Key': s3_key},
                Key=archive_file_key
            )
            logger.info(f"Archived {s3_key} to {archive_file_key}")
            end_time = datetime.datetime.now(cst)
            load_time_in_minutes = log_transfer_duration(start_time, end_time)
            #archival_audit_details = generate_audit_details(environment, source_system, batch_id_generation, "", "", bucket_name, s3_key, "", "", use_archive, archive_path, start_time, end_time, load_time_in_minutes, "archived", 0, None)
            #logger.info(f"Archived details: {archival_audit_details}")
            #all_audit_details.append(archival_audit_details)
        else:
            logger.info(f"Archiving is disabled. Skipping file archival.")
    except Exception as e:
        logger.error(f"Error archiving file {s3_key}. Error: {e}", exc_info=True)


def delete_files(bucket_name, s3_key, all_audit_details, source_system, batch_id_generation, environment):
    cst = ZoneInfo("America/Chicago")
    start_time = datetime.datetime.now(cst)
    try:
        delete_files_s3_client = boto3.client('s3')
        delete_files_s3_client.delete_object(Bucket=bucket_name, Key=s3_key)
        logger.info(f"Deleted: {s3_key}")
        end_time = datetime.datetime.now(cst)
        load_time_in_minutes = log_transfer_duration(start_time, end_time)
        #delete_audit_details = generate_audit_details(environment, source_system, batch_id_generation, "", "", bucket_name, s3_key, "", "", "", start_time, end_time, load_time_in_minutes, "deleted", 0, None)
        #logger.info(f"Deleted files details: {delete_audit_details}")
        #all_audit_details.append(delete_audit_details)
    except Exception as e:
        logger.error(f"Error deleting file {s3_key}: {e}")

# Upload log files to S3
def upload_to_s3(bucket_name, log_s3_path, data,batch_id_generation):
    try:
        # Set timezone to CST
        cst = ZoneInfo("America/Chicago")
        line_delimited_jsondata = "\n".join(json.dumps(record) for record in data)
        s3_upload_client = boto3.client('s3')
        current_time = datetime.datetime.now(cst)
        current_time_stamp_folder = current_time.strftime("%Y-%m-%d")
        file_name = f"audit_details_{batch_id_generation}.json"
        file_key = f"{log_s3_path}{current_time_stamp_folder}/{file_name}"
        logger.info(f"Uploading audit details to S3 bucket: {bucket_name}, file: {file_key}")
        s3_upload_client.put_object(Bucket=bucket_name, Key=file_key, Body=line_delimited_jsondata)
        logger.info(f"Successfully uploaded audit details to {bucket_name}/{file_key}")
    except Exception as e:
        logger.error(f"Error uploading to S3: {e}", exc_info=True)


def html_template(success_rows,failure_rows,batch_id_generation,environment):
    email_subject = f"Actuarial S3 to SMB File Transfer Summary | {environment}"
    email_body      = f"""<!DOCTYPE html>
                        <html lang="en">
                        <head>
                            <meta charset="UTF-8">
                            <meta name="viewport" content="width=device-width, initial-scale=1.0">
                            <title>File Transfer Report</title>
                            <style>
                                body {{
                                    font-family: Arial, sans-serif;
                                    margin: 20px;
                                    background-color: #f4f4f4;
                                }}
                                table {{
                                    width: 100%;
                                    border-collapse: collapse;
                                    margin-bottom: 20px;
                                }}
                                table, th, td {{
                                    border: 1px solid #ddd;
                                }}
                                th, td {{
                                    padding: 8px;
                                    text-align: left;
                                }}
                                th {{
                                    background-color: #4CAF50;
                                    color: white;
                                }}
                                tr:nth-child(even) {{
                                    background-color: #f2f2f2;
                                }}
                            </style>
                        </head>
                        <body>
                            <h2>File Transfer Report | Batch id : {batch_id_generation}</h2>

                            <h3>Successful Transfers</h3>
                            <table>
                                <thead>
                                    <tr>
                                        <th>#</th>
                                        <th>s3_path</th>
                                        <th>smb_fileshare_path</th>
                                        <th>load_time_in_minutes</th>
                                        <th>action</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    {success_rows}
                                </tbody>
                            </table>

                            <h3>Failed Transfers</h3>
                            <table>
                                <thead>
                                    <tr>
                                        <th>#</th>
                                        <th>s3_path</th>
                                        <th>smb_fileshare_path</th>
                                        <th>error</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    {failure_rows}
                                </tbody>
                            </table>

                            <p>Thank you.</p>
                            <p>Globe Life.</p>
                        </body>
                        </html>"""
    return email_subject,email_body

def generate_email_summary(email_recipients, email_subject, email_body):
    email_summary = {
        "email_recipients": email_recipients,
        "email_subject": email_subject,
        "email_body_html": email_body
    }
    return email_summary

def email_notification(email_lambda_jobname, email_payload):
    email_lambda_client = boto3.client('lambda')
    try:
        logger.info("Invoking Email Lambda job...")
        response = email_lambda_client.invoke(
            FunctionName=email_lambda_jobname,
            InvocationType="Event",
            Payload=json.dumps(email_payload)
        )
        logger.info(f"Email Lambda job '{email_lambda_jobname.split(':')[-1]}' invoked successfully.")
    except Exception as e:
        logger.error(f"Error invoking Email Lambda job: {e}")

############################################################################################################################################################


##########################################################################################################################################################
def lambda_handler(event, context):
    """
    Main function to demonstrate the usage of all functionalities:
    1. Fetch credentials from AWS Secrets Manager.
    2. Perform SMB connectivity checks.
    3. Transfer a file or folder from S3 to SMB.
    4. Log the status of the process.
    """
    try:
        cst = ZoneInfo("America/Chicago")
        start_time = datetime.datetime.now(cst)
        secret_name = os.getenv('secret_name')
        chunk_size_mb = 5
        batch_id_generation = datetime.datetime.now(cst).strftime("%Y%m%d-%H%M%S%f")
        logger.info("Starting the lambda function execution.")
        # Set timeout threshold (e.g., 10 seconds before the actual Lambda timeout)
        timeout_threshold = context.get_remaining_time_in_millis() / 1000 - 10

        # Fetching the credentials from Secrets Manager
        logger.info("Attempting to fetch credentials.")
        credentials = get_credentials(secret_name)
        if credentials:
            logger.info("Fetched secret credentials successfully.")
            username = credentials['username']
            password = credentials['password']
            logger.info(f"username : {username}")
        else:
            logger.error("Failed to fetch credentials. Exiting function.")
            return {"statusCode": 500, "body": json.dumps("Failed to fetch credentials.")}

        # Configuring SMB client
        logger.info("Configuring SMB client.")
        configure_smb(username, password)

        logger.info(f"Received event details : {event}")
        all_audit_details = []
        overall_transferred_files = set()
        overall_failed_files = set()
        email_jobname = "arn:aws:lambda:us-west-2:896172592430:function:smb-file-transfer_email_notification_test"

        for params in event['parameter']:
            logger.info(f"params : {params}")
            smb_host = params.get('smb_host', None)
            smb_port = params.get('smb_port', None)
            bucket_name = params.get('s3_bucket', None)
            s3_path = params.get('s3_path', None)
            smb_fileshare_path = params.get('smb_fileshare_path', None)
            filename_with_timestamp = params.get('filename_with_timestamp', None)
            archival = params.get('archival', None)
            archival_path = params.get('archival_path', None)
            log_s3_bucket = params.get("log_s3_bucket", None)
            log_s3_path = params.get('log_s3_path', None)
            email_recipients = params.get("business_email_recipient", None).split(",")
            source_system = params.get("source_system", None)
            environment = params.get("environment", None)
            use_retention = params.get("use_retention", None)
            retention_period = params.get("retention_period", None)
            file_prefix = params.get("file_prefix", None)
            file_extension = params.get("file_extension", None)


            if not is_port_open(smb_host, smb_port):
                msg = f"Port {smb_port} on {smb_host} is closed. Skipping"
                logger.error(msg)
                overall_failed_files.add((s3_path, msg))
                continue

            logger.info(f"Initiating file transfer from S3 to SMB.")
            transferred_files, audit_details_list, failed_files = transfer_s3_to_smb(
                bucket_name, s3_path, smb_fileshare_path, filename_with_timestamp,
                chunk_size_mb, smb_host, smb_port, archival, archival_path,
                source_system, batch_id_generation, environment
            )
            overall_transferred_files.update(transferred_files)
            overall_failed_files.update(failed_files)
            all_audit_details.extend(audit_details_list)
            
            if archival:
                for file in transferred_files:
                    archive_files(archival, bucket_name, file, archival_path, all_audit_details, source_system, batch_id_generation, environment)
                    #logger.info(f"Archived file: {file} to {archival_path}")
            # Check for timeout
            if time.time() - start_time.timestamp() > timeout_threshold:
                raise TimeoutError("Lambda function is about to timeout.")

        upload_to_s3(log_s3_bucket, log_s3_path, all_audit_details,batch_id_generation)
        logger.info(f"all_audit_details : {all_audit_details}")
        
        success_rows = ""
        overall_transferred_files = [file for file in all_audit_details if file.get("action") == 'transferred']
        if overall_transferred_files:
            for count, file in enumerate(overall_transferred_files, start=1):
                success_rows += f"<tr><td>{count}</td><td>{file['s3_path']}</td><td>{file['smb_fileshare_path']}</td><td>{file['load_time_in_minutes']}</td><td>{file['action']}</td></tr>"
        else:
            success_rows = "<tr><td colspan='5'>No files transferred successfully.</td></tr>"

        failure_rows = ""
        if overall_failed_files:
            for count, (failed_file, error) in enumerate(overall_failed_files, start=1):
                failure_rows += f"<tr><td>{count}</td><td>{failed_file}</td><td>{error}</td></tr>"
        else:
            failure_rows = "<tr><td colspan='3'>No failed files.</td></tr>"

        email_subject, email_body = html_template(success_rows, failure_rows, batch_id_generation, environment)
        lambda_event = generate_email_summary(email_recipients, email_subject, email_body)
        logger.info("Sending an email")
        email_notification(email_jobname, lambda_event)
        logger.info("Email sent successfully")

        end_time = datetime.datetime.now(cst)
        load_time_in_minutes = log_transfer_duration(start_time, end_time)
        logger.info(f"Total duration for file processing: {load_time_in_minutes}")

        return {
            'statusCode': 200,
            'body': json.dumps("Job executed successfully.")
        }

    except TimeoutError as te:
        logger.error(f"Timeout Error: {te}", exc_info=True)
        email_subject = "Actuarial S3 to SMB File Transfer Summary | Timeout Error"
        email_body = f"Your Lambda function is about to timeout."
        lambda_event = generate_email_summary(email_recipients, email_subject, email_body)
        logger.info("Sending timeout error notification email")
        email_notification(email_jobname, lambda_event)
        logger.info("Timeout error email sent successfully")
        return {
            'statusCode': 500,
            'body': json.dumps(f"Timeout Error: {te}")
        }
    except Exception as e:
        logger.error(f"An error occurred in the lambda handler: {e}", exc_info=True)
        email_subject = "Actuarial S3 to SMB File Transfer Summary | S3 Transfer Error"
        email_body = f"An error occurred during file transfer: {str(e)}"
        lambda_event = generate_email_summary(email_recipients, email_subject, email_body)
        logger.info("An error occurred during execution, email notification invoked")
        email_notification(email_jobname, lambda_event)
        logger.info("Email sent successfully")
        return {
            'statusCode': 500,
            'body': json.dumps(f"Error Message: {e}")
        }

    finally:
        logger.info('Lambda execution completed.')
    

